{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steepest descent (gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract:** _This notebook focuses on describing a steepest gradient implementation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steepest gradient is a well known optimization techniuqes. It is based on the simply assumption that the search for an optimum value (in an objective function) could be performed by directing the search towards the direction on which function tends to grow (maximize) or shrink (minimize)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the direction of steepest descent can be found by computing the derivative of changes between current and previous values of the objective function, that is $E$, with respect to each of its components:\n",
    "$\\nabla E(\\vec{w}) \\equiv \\left[ \\frac{\\delta E}{\\delta w_0}, \\frac{\\delta E}{\\delta w_1}, \\ldots, \\frac{\\delta E}{\\delta w_n} \\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient specifies the direction of steepest increase of $E$, so that the training rule for gradient descent is:\n",
    "$$\\vec{w} \\leftarrow \\vec{w} + \\Delta\\vec{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$$ \\Delta \\vec{w} = -\\eta \\nabla E(\\vec{w}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta$ is a positive constant called the _learning rate_, which determines the step size of the gradient descent search. The negative sign is needed as we want to move the weight vector in the direction that _decreases_ $E$ [^1:]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluated from individual vector components, the training rule becomes:\n",
    "$$w_i \\leftarrow w_i + \\Delta w_i$$\n",
    "So that the steepest descent is achieved by altering each component $w_i$ of $\\vec{w}$ in proportion to $\\frac{\\delta E}{\\delta w_i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, w_i [ 89.1  89.1], epsilon 3724.380000000003\n",
      "Iteration 3, w_i [ 80.19  80.19], epsilon 3016.7477999999974\n",
      "Iteration 4, w_i [ 72.171  72.171], epsilon 2443.5657180000017\n",
      "Iteration 5, w_i [ 64.9539  64.9539], epsilon 1979.2882315799998\n",
      "Iteration 6, w_i [ 58.45851  58.45851], epsilon 1603.2234675798009\n",
      "Iteration 7, w_i [ 52.612659  52.612659], epsilon 1298.6110087396364\n",
      "Iteration 8, w_i [ 47.3513931  47.3513931], epsilon 1051.874917079107\n",
      "Iteration 9, w_i [ 42.61625379  42.61625379], epsilon 852.0186828340761\n",
      "Iteration 10, w_i [ 38.35462841  38.35462841], epsilon 690.1351330956022\n",
      "Iteration 11, w_i [ 34.51916557  34.51916557], epsilon 559.0094578074372\n",
      "Iteration 12, w_i [ 31.06724901  31.06724901], epsilon 452.7976608240242\n",
      "Iteration 13, w_i [ 27.96052411  27.96052411], epsilon 366.7661052674596\n",
      "Iteration 14, w_i [ 25.1644717  25.1644717], epsilon 297.0805452666423\n",
      "Iteration 15, w_i [ 22.64802453  22.64802453], epsilon 240.63524166598017\n",
      "Iteration 16, w_i [ 20.38322208  20.38322208], epsilon 194.9145457494442\n",
      "Iteration 17, w_i [ 18.34489987  18.34489987], epsilon 157.88078205704971\n",
      "Iteration 18, w_i [ 16.51040988  16.51040988], epsilon 127.88343346621025\n",
      "Iteration 19, w_i [ 14.85936889  14.85936889], epsilon 103.58558110763033\n",
      "Iteration 20, w_i [ 13.373432  13.373432], epsilon 83.90432069718054\n",
      "Iteration 21, w_i [ 12.0360888  12.0360888], epsilon 67.96249976471626\n",
      "Iteration 22, w_i [ 10.83247992  10.83247992], epsilon 55.04962480942018\n",
      "Iteration 23, w_i [ 9.74923193  9.74923193], epsilon 44.59019609563035\n",
      "Iteration 24, w_i [ 8.77430874  8.77430874], epsilon 36.11805883746058\n",
      "Iteration 25, w_i [ 7.89687786  7.89687786], epsilon 29.255627658343073\n",
      "Iteration 26, w_i [ 7.10719008  7.10719008], epsilon 23.6970584032579\n",
      "Iteration 27, w_i [ 6.39647107  6.39647107], epsilon 19.194617306638904\n",
      "Iteration 28, w_i [ 5.75682396  5.75682396], epsilon 15.547640018377507\n",
      "Iteration 29, w_i [ 5.18114157  5.18114157], epsilon 12.593588414885765\n",
      "Iteration 30, w_i [ 4.66302741  4.66302741], epsilon 10.200806616057477\n",
      "Iteration 31, w_i [ 4.19672467  4.19672467], epsilon 8.26265335900655\n",
      "Iteration 32, w_i [ 3.7770522  3.7770522], epsilon 6.692749220795317\n",
      "Iteration 33, w_i [ 3.39934698  3.39934698], epsilon 5.421126868844205\n",
      "Iteration 34, w_i [ 3.05941228  3.05941228], epsilon 4.391112763763807\n",
      "Iteration 35, w_i [ 2.75347106  2.75347106], epsilon 3.5568013386486825\n",
      "Iteration 36, w_i [ 2.47812395  2.47812395], epsilon 2.881009084305431\n",
      "Iteration 37, w_i [ 2.23031155  2.23031155], epsilon 2.333617358287402\n",
      "Iteration 38, w_i [ 2.0072804  2.0072804], epsilon 1.8902300602127955\n",
      "Iteration 39, w_i [ 1.80655236  1.80655236], epsilon 1.5310863487723632\n",
      "Iteration 40, w_i [ 1.62589712  1.62589712], epsilon 1.2401799425056135\n",
      "Iteration 41, w_i [ 1.46330741  1.46330741], epsilon 1.0045457534295466\n",
      "Iteration 42, w_i [ 1.31697667  1.31697667], epsilon 0.8136820602779329\n",
      "Iteration 43, w_i [ 1.185279  1.185279], epsilon 0.659082468825126\n",
      "Iteration 44, w_i [ 1.0667511  1.0667511], epsilon 0.5338567997483521\n",
      "Iteration 45, w_i [ 0.96007599  0.96007599], epsilon 0.43242400779616563\n",
      "Iteration 46, w_i [ 0.86406839  0.86406839], epsilon 0.3502634463148937\n",
      "Iteration 47, w_i [ 0.77766155  0.77766155], epsilon 0.2837133915150638\n",
      "Iteration 48, w_i [ 0.6998954  0.6998954], epsilon 0.22980784712720181\n",
      "Iteration 49, w_i [ 0.62990586  0.62990586], epsilon 0.18614435617303338\n",
      "Iteration 50, w_i [ 0.56691527  0.56691527], epsilon 0.15077692850015711\n",
      "Iteration 51, w_i [ 0.51022375  0.51022375], epsilon 0.12212931208512734\n",
      "Iteration 52, w_i [ 0.45920137  0.45920137], epsilon 0.09892474278895313\n",
      "Iteration 53, w_i [ 0.41328123  0.41328123], epsilon 0.08012904165905194\n",
      "Iteration 54, w_i [ 0.37195311  0.37195311], epsilon 0.06490452374383221\n",
      "Iteration 55, w_i [ 0.3347578  0.3347578], epsilon 0.05257266423250401\n",
      "Iteration 56, w_i [ 0.30128202  0.30128202], epsilon 0.04258385802832826\n",
      "Iteration 57, w_i [ 0.27115382  0.27115382], epsilon 0.0344929250029459\n",
      "Iteration 58, w_i [ 0.24403844  0.24403844], epsilon 0.02793926925238613\n",
      "Iteration 59, w_i [ 0.21963459  0.21963459], epsilon 0.022630808094432783\n",
      "Iteration 60, w_i [ 0.19767113  0.19767113], epsilon 0.018330954556490545\n",
      "Iteration 61, w_i [ 0.17790402  0.17790402], epsilon 0.014848073190757358\n",
      "Iteration 62, w_i [ 0.16011362  0.16011362], epsilon 0.012026939284513441\n",
      "Iteration 63, w_i [ 0.14410226  0.14410226], epsilon 0.009741820820455896\n",
      "Iteration 64, w_i [ 0.12969203  0.12969203], epsilon 0.00789087486456927\n",
      "Iteration 65, w_i [ 0.11672283  0.11672283], epsilon 0.006391608640301114\n",
      "Iteration 66, w_i [ 0.10505054  0.10505054], epsilon 0.005177202998643899\n",
      "Iteration 67, w_i [ 0.09454549  0.09454549], epsilon 0.004193534428901562\n",
      "Iteration 68, w_i [ 0.08509094  0.08509094], epsilon 0.0033967628874102627\n",
      "Iteration 69, w_i [ 0.07658185  0.07658185], epsilon 0.002751377938802313\n",
      "Iteration 70, w_i [ 0.06892366  0.06892366], epsilon 0.0022286161304298727\n",
      "Iteration 71, w_i [ 0.0620313  0.0620313], epsilon 0.0018051790656481983\n",
      "Iteration 72, w_i [ 0.05582817  0.05582817], epsilon 0.0014621950431750394\n",
      "Iteration 73, w_i [ 0.05024535  0.05024535], epsilon 0.0011843779849717833\n",
      "Iteration 74, w_i [ 0.04522081  0.04522081], epsilon 0.0009593461678271435\n",
      "Iteration 75, w_i [ 0.04069873  0.04069873], epsilon 0.000777070395939986\n",
      "Iteration 76, w_i [ 0.03662886  0.03662886], epsilon 0.0006294270207113889\n",
      "Iteration 77, w_i [ 0.03296597  0.03296597], epsilon 0.0005098358867762251\n",
      "Iteration 78, w_i [ 0.02966938  0.02966938], epsilon 0.0004129670682887427\n",
      "Iteration 79, w_i [ 0.02670244  0.02670244], epsilon 0.0003345033253138814\n",
      "Iteration 80, w_i [ 0.0240322  0.0240322], epsilon 0.00027094769350424373\n",
      "Iteration 81, w_i [ 0.02162898  0.02162898], epsilon 0.0002194676317384377\n",
      "Iteration 82, w_i [ 0.01946608  0.01946608], epsilon 0.00017776878170813434\n",
      "Iteration 83, w_i [ 0.01751947  0.01751947], epsilon 0.0001439927131835889\n",
      "Iteration 84, w_i [ 0.01576752  0.01576752], epsilon 0.00011663409767870704\n",
      "Iteration 85, w_i [ 0.01419077  0.01419077], epsilon 9.447361911975267e-05\n",
      "Iteration 86, w_i [ 0.01277169  0.01277169], epsilon 7.652363148699968e-05\n",
      "Iteration 87, w_i [ 0.01149452  0.01149452], epsilon 6.198414150446972e-05\n",
      "Iteration 88, w_i [ 0.01034507  0.01034507], epsilon 5.020715461862051e-05\n",
      "Iteration 89, w_i [ 0.00931056  0.00931056], epsilon 4.066779524108255e-05\n",
      "Iteration 90, w_i [ 0.00837951  0.00837951], epsilon 3.2940914145276884e-05\n",
      "Iteration 91, w_i [ 0.00754156  0.00754156], epsilon 2.66821404576743e-05\n",
      "Iteration 92, w_i [ 0.0067874  0.0067874], epsilon 2.1612533770716167e-05\n",
      "Iteration 93, w_i [ 0.00610866  0.00610866], epsilon 1.7506152354280116e-05\n",
      "Iteration 94, w_i [ 0.0054978  0.0054978], epsilon 1.4179983406966876e-05\n",
      "Iteration 95, w_i [ 0.00494802  0.00494802], epsilon 1.1485786559643185e-05\n",
      "Iteration 96, w_i [ 0.00445321  0.00445321], epsilon 9.30348711331096e-06\n",
      "Minimum ws is [ 0.00445321  0.00445321]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_obj_fun():\n",
    "    return lambda x1,x2: (x1**2) + (x2**2)\n",
    "\n",
    "def evaluate_gradient(ws):\n",
    "    return np.array([2*ws[0], 2*ws[1]])\n",
    "\n",
    "stop_criterion = 0.00001\n",
    "epsilon = 1\n",
    "\n",
    "fun = get_obj_fun()\n",
    "alpha = 0.05\n",
    "ws = np.array([99,99])\n",
    "\n",
    "search_vector = evaluate_gradient(ws)\n",
    "fun_previous = fun(ws[0], ws[1])\n",
    "i = 1\n",
    "while epsilon > stop_criterion:\n",
    "    ws = ws - (alpha * search_vector)\n",
    "    search_vector = evaluate_gradient(ws)\n",
    "    fun_current = fun(ws[0], ws[1])\n",
    "    epsilon = np.fabs(fun_current - fun_previous)\n",
    "    fun_previous = fun_current\n",
    "    i += 1\n",
    "\n",
    "    print('Iteration {}, w_i {}, epsilon {}'.format(i, ws, epsilon))\n",
    "\n",
    "\n",
    "print('Minimum ws is {}'.format(ws))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Footnotes:**\n",
    "\n",
    "1. Recall that $\\nabla E$ (the gradient vector) _points towards_ the direction of increase, thus the negative sign."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
